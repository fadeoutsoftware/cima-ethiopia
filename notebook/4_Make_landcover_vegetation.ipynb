{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea84de6",
   "metadata": {},
   "source": [
    "## 🔧 Preliminary Setup\n",
    "Clarify the configuration and creates the folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5a584",
   "metadata": {},
   "source": [
    "#### Specify Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de29f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using settings file: danakil.json\n"
     ]
    }
   ],
   "source": [
    "# Reuse the same simple JSON used in the DEM/land notebook\n",
    "settings_file = \"danakil.json\"  \n",
    "print(f\"Using settings file: {settings_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbd9f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3305ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded convertAIIGrid from libraries.io_tools\n"
     ]
    }
   ],
   "source": [
    "import os, json, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from osgeo import gdal, gdalconst\n",
    "from pathlib import Path\n",
    "#from scipy import ndimage as nd\n",
    "\n",
    "\n",
    "# ✅ Import the existing converter from libraries.io_tools (as in your other notebook)\n",
    "from io_tools import convertAIIGrid\n",
    "print(\"Loaded convertAIIGrid from libraries.io_tools\")\n",
    "\n",
    "from geo_tools import rasterize_vector_to_model, regrid_to_model\n",
    "from io_tools import write_tif_from_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3716f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create work folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4e8ec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from settings file: danakil.json\n",
      "Domain: danakil\n",
      "Output: projects/training_eth/output/danakil\n",
      "Ancillary: projects/training_eth/ancillary\n",
      "DEM: projects/training_eth/output/danakil/danakil.dem.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the project root directory\n",
    "project_root = str(Path().cwd()).replace('notebook','')\n",
    "\n",
    "# Load configuration from the YAML file\n",
    "with open(os.path.join(project_root, \"settings\", settings_file), 'r') as f:\n",
    "    config = json.load(f)\n",
    "    cfg = config.copy()\n",
    "    \n",
    "print(f\"Configuration loaded from settings file: {settings_file}\")\n",
    "\n",
    "\n",
    "# Generate folder tree based on the configuration\n",
    "path_settings = config['path']\n",
    "for key in path_settings.keys():\n",
    "    path_settings[key] = os.path.join(project_root, \"projects\", config['general']['project'], path_settings[key])\n",
    "\n",
    "domain = config[\"general\"][\"domain\"]\n",
    "dem_path  = os.path.join(path_settings[\"output\"], domain + \".dem.txt\")\n",
    "\n",
    "print(\"Domain:\", domain)\n",
    "print(\"Output:\", path_settings[\"output\"].replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"Ancillary:\", path_settings[\"ancillary\"].replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"DEM:\", dem_path.replace(\"/home/continuumuser/workdir/\",\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe5316",
   "metadata": {},
   "source": [
    "#### Local dataset files (edit if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "089d5419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sources/datasets/Land_Soil/maps/C3S-ESACII-LC-ETH.tif → OK\n",
      "sources/datasets/Land_Soil/maps/average_sand_ETH_cpr.tif → OK\n",
      "sources/datasets/Land_Soil/maps/average_clay_ETH_cpr.tif → OK\n",
      "sources/datasets/Land_Soil/tables/ESA_CCI_to_CN.csv → OK\n",
      "sources/datasets/Land_Soil/tables/ESA_CCI_to_veg.csv → OK\n"
     ]
    }
   ],
   "source": [
    "LAND_SOIL_DIR = os.path.join(project_root, \"sources/datasets/Land_Soil\")\n",
    "LC_MAP   = os.path.join(LAND_SOIL_DIR, \"maps\",  \"C3S-ESACII-LC-ETH.tif\")\n",
    "SOIL_CONT_MAPS = {}\n",
    "SOIL_CONT_MAPS[\"sand\"] = os.path.join(LAND_SOIL_DIR, \"maps\",  \"average_sand_ETH_cpr.tif\")\n",
    "SOIL_CONT_MAPS[\"clay\"] = os.path.join(LAND_SOIL_DIR, \"maps\",  \"average_clay_ETH_cpr.tif\")\n",
    "TABLE_CN = os.path.join(LAND_SOIL_DIR, \"tables\",\"ESA_CCI_to_CN.csv\")\n",
    "TABLE_VEG= os.path.join(LAND_SOIL_DIR, \"tables\",\"ESA_CCI_to_veg.csv\")\n",
    "\n",
    "for p in [LC_MAP, SOIL_CONT_MAPS[\"sand\"], SOIL_CONT_MAPS[\"clay\"], TABLE_CN, TABLE_VEG]:\n",
    "    print(p.replace(\"/home/continuumuser/workdir/\",\"\"), \"→\", \"OK\" if os.path.exists(p) else \"MISSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99e512",
   "metadata": {},
   "source": [
    "## 🧩 Soil Tools\n",
    "Declare a set of function for soil classification that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "329029cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(data, invalid=None):\n",
    "    \"\"\"Fill nodata/zero values using pure NumPy nearest neighbour.\"\"\"\n",
    "    if invalid is None:\n",
    "        invalid = (np.isnan(data)) | (data == -9999) | (data == 0)\n",
    "    valid_mask = ~invalid\n",
    "    if not np.any(valid_mask):\n",
    "        return data\n",
    "    # Get coordinates of valid pixels\n",
    "    valid_yx = np.argwhere(valid_mask)\n",
    "    valid_vals = data[valid_mask]\n",
    "    # Get coordinates of all pixels\n",
    "    all_yx = np.indices(data.shape).reshape(2, -1).T\n",
    "    filled_data = data.copy().reshape(-1)\n",
    "    for idx, (y, x) in enumerate(all_yx):\n",
    "        if invalid[y, x]:\n",
    "            # Compute squared distances to valid pixels\n",
    "            dist2 = (valid_yx[:, 0] - y)**2 + (valid_yx[:, 1] - x)**2\n",
    "            nearest_idx = np.argmin(dist2)\n",
    "            filled_data[idx] = valid_vals[nearest_idx]\n",
    "    return filled_data.reshape(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f104d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil type classification\n",
    "def classify_usda(out_map, sand, clay, silt):\n",
    "    out_map = np.where((clay >= 0.4) & (sand <= 0.45) & (silt < 0.4), 1, out_map)   # clay\n",
    "    out_map = np.where((clay >= 0.4) & (silt >= 0.4), 2, out_map)                   # silty-clay\n",
    "    out_map = np.where((clay >= 0.27) & (clay < 0.4) & (sand <= 0.2), 3, out_map)   # silty-clay-loam\n",
    "    out_map = np.where((clay >= 0.35) & (sand >= 0.45), 4, out_map)                 # sandy-clay\n",
    "    out_map = np.where((clay >= 0.2) & (clay < 0.35) & (silt < 0.28) & (sand > 0.45), 5, out_map) # sandy-clay-loam\n",
    "    out_map = np.where((clay >= 0.27) & (clay < 0.4) & (sand > 0.2) & (sand <= 0.45), 6, out_map) # clay-loam\n",
    "    out_map = np.where((silt >= 0.8) & (clay < 0.12), 7, out_map)                    # silt\n",
    "    out_map = np.where(((silt >= 0.5) & (clay >= 0.12) & (clay < 0.27)) | ((silt >= 0.5) & (silt < 0.8) & (clay < 0.12)), 8, out_map) # silt-loam\n",
    "    out_map = np.where((clay >= 0.07) & (clay <= 0.27) & (silt >= 0.28) & (silt < 0.5) & (sand <= 0.52), 9, out_map) # loam\n",
    "    out_map = np.where((silt + 1.5 * clay) < 0.15, 10, out_map)                      # sand\n",
    "    out_map = np.where(((silt + 1.5 * clay) >= 0.15) & ((silt + 2 * clay) < 0.3), 11, out_map) # loamy-sand\n",
    "    out_map = np.where(((clay >= 0.07) & (clay <= 0.2) & (sand > 0.52) & ((silt + 2 * clay) >= 0.3)) |\n",
    "                       ((clay < 0.07) & (silt < 0.5) & ((silt + 2 * clay) >= 0.3)), 12, out_map) # sandy-loam\n",
    "    out_map = np.where(sand == 0, -9999, out_map)  # guard for nodata\n",
    "    return out_map\n",
    "\n",
    "# Hydrological soil group identification\n",
    "def classify_hsg(out_map, soil_class):\n",
    "    out_map = np.where(soil_class == 10, 1, out_map)   # A\n",
    "    out_map = np.where((soil_class == 11) | (soil_class == 12), 2, out_map)  # B\n",
    "    out_map = np.where(((soil_class > 4) & (soil_class < 10)) | (soil_class == 3), 3, out_map)  # C\n",
    "    out_map = np.where((soil_class == 1) | (soil_class == 2) | (soil_class == 4), 4, out_map)   # D\n",
    "    return out_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca6d24",
   "metadata": {},
   "source": [
    "## 🗺️ Regrid maps on model grid\n",
    "#### Read reference grid from the model DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58fbc8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid: 640 x 734\n"
     ]
    }
   ],
   "source": [
    "from osgeo import osr\n",
    "\n",
    "def epsg_to_wkt(epsg_code: str) -> str:\n",
    "    s = osr.SpatialReference()\n",
    "    s.ImportFromEPSG(int(epsg_code.split(\":\")[1]))\n",
    "    return s.ExportToWkt()\n",
    "\n",
    "match_ds = gdal.Open(dem_path, gdalconst.GA_ReadOnly)\n",
    "out_profile = rio.open(dem_path).profile\n",
    "out_profile[\"driver\"] = \"GTiff\"\n",
    "\n",
    "dem = np.array(match_ds.GetRasterBand(1).ReadAsArray())\n",
    "geotrans = match_ds.GetGeoTransform()\n",
    "wide = match_ds.RasterXSize\n",
    "high = match_ds.RasterYSize\n",
    "\n",
    "proj_wkt_dem = match_ds.GetProjection() \n",
    "proj_wkt = proj_wkt_dem.strip() if proj_wkt_dem and proj_wkt_dem.strip() else epsg_to_wkt(\"EPSG:4326\")\n",
    "\n",
    "# bbox DEM\n",
    "lon_min = min(geotrans[0], geotrans[0] + wide * geotrans[1])\n",
    "lon_max = max(geotrans[0], geotrans[0] + wide * geotrans[1])\n",
    "lat_min = min(geotrans[3], geotrans[3] + high * geotrans[5])\n",
    "lat_max = max(geotrans[3], geotrans[3] + high * geotrans[5])\n",
    "\n",
    "model_srs = {\n",
    "    \"proj\": proj_wkt,  \n",
    "    \"geotrans\": geotrans,\n",
    "    \"wide\": wide,\n",
    "    \"high\": high,\n",
    "    \"bbox\": [lon_min, lon_max, lat_min, lat_max], \n",
    "}\n",
    "print(\"Grid:\", wide, \"x\", high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f08a29",
   "metadata": {},
   "source": [
    "#### Regrid Local Maps (ESA CCI, sand, clay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03219acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrid done → LULC_regrid.tif average_regrid_sand.tif average_regrid_clay.tif\n"
     ]
    }
   ],
   "source": [
    "anc_lc   = os.path.join(path_settings[\"ancillary\"], \"LULC_regrid.tif\")\n",
    "anc_sand = os.path.join(path_settings[\"ancillary\"], \"average_regrid_sand.tif\")\n",
    "anc_clay = os.path.join(path_settings[\"ancillary\"], \"average_regrid_clay.tif\")\n",
    "\n",
    "#src = gdal.Open(LC_MAP); src_wkt = src.GetProjection()\n",
    "#write_tif_from_grid(src_wkt, model_srs, LC_MAP, anc_lc, gdalconst.GRA_Mode, file_type=gdalconst.GDT_Float32)\n",
    "\n",
    "#src = gdal.Open(SAND_MAP); src_wkt = src.GetProjection()\n",
    "#write_tif_from_grid(src_wkt, model_srs, SAND_MAP, anc_sand, gdalconst.GRA_Bilinear, file_type=gdalconst.GDT_Float32)\n",
    "\n",
    "#src = gdal.Open(CLAY_MAP); src_wkt = src.GetProjection()\n",
    "#write_tif_from_grid(src_wkt, model_srs, CLAY_MAP, anc_clay, gdalconst.GRA_Bilinear, file_type=gdalconst.GDT_Float32)\n",
    "\n",
    "#print(\"Regrid done →\", os.path.basename(anc_lc), os.path.basename(anc_sand), os.path.basename(anc_clay))\n",
    "\n",
    "regrid_to_model(LC_MAP,   anc_lc,   model_srs, resample_alg=gdalconst.GRA_Mode,     dst_nodata=-9999.0)\n",
    "regrid_to_model(SOIL_CONT_MAPS[\"sand\"], anc_sand, model_srs, resample_alg=gdalconst.GRA_Bilinear, dst_nodata=-9999.0)\n",
    "regrid_to_model(SOIL_CONT_MAPS[\"clay\"], anc_clay, model_srs, resample_alg=gdalconst.GRA_Bilinear, dst_nodata=-9999.0)\n",
    "\n",
    "print(\"Regrid done →\", os.path.basename(anc_lc), os.path.basename(anc_sand), os.path.basename(anc_clay))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13db87",
   "metadata": {},
   "source": [
    "#### Read Lake mask (if used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "563c9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake mask ready: /home/continuumuser/workdir/projects/training_eth/ancillary/lake_mask_regrid.tif\n"
     ]
    }
   ],
   "source": [
    "# Lake mask (raster o vettore). Flag in cfg.flags o cfg.algorithm.flags\n",
    "if \"algorithm\" in cfg and \"flags\" in cfg[\"algorithm\"]:\n",
    "    use_lake_mask = bool(cfg[\"algorithm\"][\"flags\"].get(\"use_lake_mask\", False))\n",
    "else:\n",
    "    use_lake_mask = bool(cfg.get(\"flags\", {}).get(\"use_lake_mask\", False))\n",
    "\n",
    "lake_entry = cfg.get(\"data\", {}).get(\"lake_mask\", None)\n",
    "lake_mask = None\n",
    "\n",
    "if not use_lake_mask or not lake_entry:\n",
    "    print(\"Lake mask not used.\")\n",
    "else:\n",
    "    lake_path = lake_entry if os.path.isabs(lake_entry) else os.path.join(path_settings[\"data\"], lake_entry)\n",
    "    if not os.path.exists(lake_path):\n",
    "        print(f\"Lake mask not found: {lake_path}\")\n",
    "    else:\n",
    "        ext = os.path.splitext(lake_path)[1].lower()\n",
    "        is_vector = ext in [\".shp\", \".gpkg\", \".geojson\", \".json\"]\n",
    "        tmp_lake = os.path.join(path_settings[\"ancillary\"], \"lake_mask_regrid.tif\")\n",
    "        if is_vector:\n",
    "            rasterize_vector_to_model(lake_path, tmp_lake, model_srs, burn_value=1, dst_nodata=0, assumed_src_epsg=\"EPSG:4326\")\n",
    "        else:\n",
    "            regrid_to_model(lake_path, tmp_lake, model_srs, resample_alg=gdalconst.GRA_NearestNeighbour, dst_nodata=0)\n",
    "        lake_mask = rio.open(tmp_lake).read(1).astype(int)\n",
    "        lake_mask = (lake_mask != 0).astype(int)\n",
    "        print(\"Lake mask ready:\", tmp_lake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d4e48",
   "metadata": {},
   "source": [
    "## 🧮 Calculate soil maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49b18ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Soil volume and CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d09c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN & S exported to: projects/training_eth/output/danakil\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SAND = 40.0\n",
    "DEFAULT_CLAY = 20.0\n",
    "\n",
    "# Sand\n",
    "sand_map_raw = rio.open(anc_sand).read(1).astype(np.float32)\n",
    "sand_map = np.where((sand_map_raw == -9999) | (sand_map_raw == 0) | np.isnan(sand_map_raw),\n",
    "                    DEFAULT_SAND, sand_map_raw)/10\n",
    "\n",
    "# Clay\n",
    "clay_map_raw = rio.open(anc_clay).read(1).astype(np.float32)\n",
    "clay_map = np.where((clay_map_raw == -9999) | (clay_map_raw == 0) | np.isnan(clay_map_raw),\n",
    "                    DEFAULT_CLAY, clay_map_raw)/10\n",
    "\n",
    "# Silt\n",
    "silt = 100.0 - sand_map - clay_map\n",
    "\n",
    "# USDA soil classification\n",
    "soil_class = -9999 * np.ones_like(sand_map, dtype=np.float32)\n",
    "soil_class = classify_usda(soil_class, sand_map/100.0, clay_map/100.0, silt/100.0)\n",
    "soil_class[dem < -9000] = -9999\n",
    "\n",
    "# Hydrological Soil Group classification\n",
    "hsg = -9999 * np.ones_like(dem, dtype=np.float32)\n",
    "hsg = classify_hsg(hsg, soil_class)\n",
    "\n",
    "# CN lookup by ESA class + HSG using NumPy lookup table\n",
    "lulc_df = pd.read_csv(TABLE_CN, header=0, usecols=[\"ID\",\"A\",\"B\",\"C\",\"D\"], index_col=\"ID\", sep=',')\n",
    "max_id = int(lulc_df.index.max())\n",
    "lut = np.full((max_id + 1, 4), -9999, dtype=np.float32)\n",
    "lut[lulc_df.index.values] = lulc_df[[\"A\", \"B\", \"C\", \"D\"]].values\n",
    "\n",
    "lulc_map = rio.open(anc_lc).read(1).astype(np.int32)\n",
    "\n",
    "cn_map = np.full_like(dem, -9999, dtype=np.float32)\n",
    "for num, col_idx in enumerate([0, 1, 2, 3], start=1):\n",
    "    vals = lut[lulc_map, col_idx]\n",
    "    cn_map = np.where(hsg == num, vals, cn_map)\n",
    "\n",
    "# Compute S only for valid CN values\n",
    "valid_cn = (cn_map > 0) & (cn_map <= 100)\n",
    "S = np.full_like(cn_map, -9999, dtype=np.float32)\n",
    "S[valid_cn] = 254.0 * ((100.0 / cn_map[valid_cn]) - 1.0)\n",
    "\n",
    "# Apply lake mask to CN and S\n",
    "cn_map = np.where(lake_mask == 1, 5, cn_map)\n",
    "S = np.where(lake_mask == 1, 4500, S)\n",
    "\n",
    "# Temp GeoTIFFs\n",
    "t_cn = os.path.join(path_settings[\"ancillary\"], \"temp_cn.tif\")\n",
    "t_S  = os.path.join(path_settings[\"ancillary\"], \"temp_S.tif\")\n",
    "with rio.open(t_cn, 'w', **out_profile) as dst:\n",
    "    dst.write(cn_map, 1)\n",
    "with rio.open(t_S, 'w', **out_profile) as dst:\n",
    "    dst.write(S, 1)\n",
    "\n",
    "# Export to ASCII grid\n",
    "convertAIIGrid(t_cn, os.path.join(path_settings[\"output\"], f\"{domain}.cn.txt\"), outType=\"Int16\", precision=0)\n",
    "convertAIIGrid(t_S, os.path.join(path_settings[\"output\"], f\"{domain}.soil_vmax.txt\"), outType=\"Float32\", precision=2)\n",
    "\n",
    "print(\"CN & S exported to:\", path_settings[\"output\"].replace(\"/home/continuumuser/workdir/\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04e3bf",
   "metadata": {},
   "source": [
    "#### Soil Hydraulic Properties (ct, infilt, soil_ksat_drain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1aa2a610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct, soil_ksat_infilt, soil_ksat_drain exported to: projects/training_eth/output/danakil\n"
     ]
    }
   ],
   "source": [
    "# Saxton et al. (1986) PTFs using sand/clay (%)\n",
    "sand = sand_map.astype(np.float32)\n",
    "clay = clay_map.astype(np.float32)\n",
    "\n",
    "a = np.exp(-4.396 - 0.0715*clay - 0.000488*(sand**2) - 0.00004285*(sand**2)*clay)\n",
    "b = -3.14 - 0.00222*(clay**2) - 0.00003484*(sand**2)*clay\n",
    "ct = (0.33333/a)**(1.0/b)  # field capacity (theta at 1/3 bar)\n",
    "\n",
    "porosity = 0.332 - 0.0007251*sand + 0.1276*np.log10(np.maximum(clay, 1e-6))\n",
    "ks_infilt = np.exp((12.012 - 0.0755*sand) + (-3.895 + 0.03671*sand - 0.1103*clay + 0.00087546*(clay**2)) / np.maximum(porosity, 1e-6))\n",
    "ks_drain = ks_infilt.copy()\n",
    "\n",
    "# Apply lake mask overrides if present\n",
    "if 'lake_mask' in globals() and isinstance(lake_mask, np.ndarray) and use_lake_mask:\n",
    "    ks_infilt = np.where(lake_mask == 1, 100.0, ks_infilt)\n",
    "    ks_drain  = np.where(lake_mask == 1, 0.03, ks_drain)\n",
    "    ct        = np.where(lake_mask == 1, 0.9, ct)\n",
    "\n",
    "# Save temp tifs\n",
    "t_ct = os.path.join(path_settings[\"ancillary\"], \"temp_ct.tif\")\n",
    "t_k1 = os.path.join(path_settings[\"ancillary\"], \"temp_ksat_infilt.tif\")\n",
    "t_k2 = os.path.join(path_settings[\"ancillary\"], \"temp_ksat_drain.tif\")\n",
    "with rio.open(t_ct, 'w', **out_profile) as dst: dst.write(ct.astype(np.float32), 1)\n",
    "with rio.open(t_k1, 'w', **out_profile) as dst: dst.write(ks_infilt.astype(np.float32), 1)\n",
    "with rio.open(t_k2, 'w', **out_profile) as dst: dst.write(ks_drain.astype(np.float32), 1)\n",
    "\n",
    "# Export AAIGrid\n",
    "convertAIIGrid(t_ct, os.path.join(path_settings[\"output\"], f\"{domain}.ct.txt\"), outType=\"Float32\", precision=2)\n",
    "convertAIIGrid(t_k1, os.path.join(path_settings[\"output\"], f\"{domain}.soil_ksat_infilt.txt\"), outType=\"Float32\", precision=2)\n",
    "convertAIIGrid(t_k2, os.path.join(path_settings[\"output\"], f\"{domain}.soil_ksat_drain.txt\"), outType=\"Float32\", precision=2)\n",
    "\n",
    "print(\"ct, soil_ksat_infilt, soil_ksat_drain exported to:\", path_settings[\"output\"].replace(\"/home/continuumuser/workdir/\",\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf531c",
   "metadata": {},
   "source": [
    "## 🌱 Vegetation Layers (BareSoil, RSmin, Hveg, Gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57080a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vegetation maps exported to: projects/training_eth/output/danakil\n"
     ]
    }
   ],
   "source": [
    "lulc_to_veg = pd.read_csv(TABLE_VEG, header=0, usecols=[\"ID\",\"BareSoil\",\"RSmin\",\"Hveg\",\"Gd\"], index_col=\"ID\", sep=',')\n",
    "lulc = rio.open(anc_lc).read(1).astype(np.int32)\n",
    "\n",
    "for var, outType, precision in [\n",
    "    (\"BareSoil\", \"Int16\",   0),\n",
    "    (\"RSmin\",    \"Float32\", 2),\n",
    "    (\"Hveg\",     \"Float32\", 2),\n",
    "    (\"Gd\",       \"Float32\", 2),\n",
    "]:\n",
    "    arr = lulc_to_veg.reindex(lulc.flatten())[var].fillna(-9999).values.reshape(dem.shape).astype(np.float32)\n",
    "    arr[dem < -9000] = -9999\n",
    "    t_var = os.path.join(path_settings[\"ancillary\"], f\"temp_{var}.tif\")\n",
    "    with rio.open(t_var, 'w', **out_profile) as dst: dst.write(arr, 1)\n",
    "    convertAIIGrid(t_var, os.path.join(path_settings[\"output\"], f\"{domain}.{var}.txt\"), outType=outType, precision=precision)\n",
    "\n",
    "print(\"Vegetation maps exported to:\", path_settings[\"output\"].replace(\"/home/continuumuser/workdir/\",\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376602b1",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a69744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup done.\n"
     ]
    }
   ],
   "source": [
    "# Remove auxiliary files some GDAL versions might leave\n",
    "os.system(f'find \"{path_settings[\"output\"]}\" -maxdepth 1 -type f -name \"*.xml\" -delete || true')\n",
    "print(\"Cleanup done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758cc989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
