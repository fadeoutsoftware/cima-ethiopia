{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8de2f7",
   "metadata": {},
   "source": [
    "\n",
    "# üìç Make Continuum Point Files (Dams, Lakes, Sections)\n",
    "\n",
    "<br>\n",
    "<img style=\"float: left; padding-right: 15px; padding-left: 0px;\" src=\"../sources/images/logo_continuum.png\" width=\"260px\" align=‚Äùleft‚Äù >\n",
    "\n",
    "<div style=\"text-align: justify\">This is a Jupyter Notebook, a web-based interactive development environment that allows to create and share python codes.\n",
    "This notebook loads parameters from a configuration file (`config.json`), saved in the `settings` folder and do a set of operations for the preparations of the static point data for Continuum model:\n",
    "\n",
    "- uses the dams extracted from the GranD database to build the info_dam.txt files\n",
    "- uses the lakes extracted from HydroSheds database to build the info_lake.txt file\n",
    "- uses a set of points to build the info_section.txt file\n",
    "\n",
    "The entire workflow is documented with clear explanations and visual plots to help you understand each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f228b7",
   "metadata": {},
   "source": [
    "## üîß Preliminary Setup ‚Äî Select Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a247d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using settings: danakil_point.json\n"
     ]
    }
   ],
   "source": [
    "# Usa un JSON semplice e unificato\n",
    "settings_file = \"danakil_point.json\"  # puoi cambiarlo qui\n",
    "\n",
    "print(\"Using settings:\", settings_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804bfea",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f21944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import os, json, logging, math\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from plot_tools import single_plot\n",
    "from osgeo import gdal, gdalconst\n",
    "\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982860d9",
   "metadata": {},
   "source": [
    "#### Setup the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474a1134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: danakil\n",
      "Data: projects/training_eth/data\n",
      "Output: projects/training_eth/output/danakil\n",
      "choice: projects/training_eth/data/danakil.choice.txt\n",
      "pnt   : projects/training_eth/data/danakil.pnt.txt\n",
      "dams  : projects/training_eth/data/dams_in.shp\n",
      "lakes : projects/training_eth/data/lakes_in.shp\n",
      "sects : projects/training_eth/data/danakil_river_flow.shp\n"
     ]
    }
   ],
   "source": [
    "# Define the project root directory\n",
    "project_root = str(Path().cwd()).replace('notebook','')\n",
    "\n",
    "# Load configuration from the YAML file\n",
    "with open(os.path.join(project_root, \"settings\", settings_file), 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "domain = cfg[\"general\"][\"domain\"]\n",
    "# Generate folder tree based on the configuration\n",
    "path_settings = cfg['path']\n",
    "for key in path_settings.keys():\n",
    "    path_settings[key] = os.path.join(project_root, \"projects\", cfg['general']['project'], path_settings[key])\n",
    "    os.makedirs(path_settings[key], exist_ok=True)\n",
    "\n",
    "# Input rasters (grid context)\n",
    "choice_path = os.path.join(path_settings[\"data\"], cfg[\"data\"][\"choice\"])\n",
    "pnt_path    = os.path.join(path_settings[\"data\"], cfg[\"data\"][\"pnt\"])\n",
    "\n",
    "# Shapefiles\n",
    "dams_file     = os.path.join(path_settings[\"data\"], cfg[\"data\"][\"dam_points\"])\n",
    "lakes_file    = os.path.join(path_settings[\"data\"], cfg[\"data\"][\"lake_points\"])\n",
    "sections_file = os.path.join(path_settings[\"data\"], cfg[\"data\"][\"section_points\"])\n",
    "\n",
    "# Dams and lakes standard names\n",
    "dams_lut = {\n",
    "      \"name_col\": \"DAM_NAME\",\n",
    "      \"year_col\": \"YEAR\",\n",
    "      \"cap_col\": \"CAP_MCM\",\n",
    "      \"len_col\": \"DAM_LEN_M\",\n",
    "      \"height_col\": \"DAM_HGT_M\",\n",
    "      \"area_col\": \"AREA_SKM\",\n",
    "      \"dis_avg_col\": \"DIS_AVG_LS\"\n",
    "    }\n",
    "lakes_lut = {\n",
    "      \"name_col\": \"Lake_name\",\n",
    "      \"res_time_col\": \"Res_time\",\n",
    "      \"vol_total_col\": \"Vol_total\",\n",
    "      \"dis_avg_col\": \"Dis_avg\"\n",
    "    }\n",
    "        \n",
    "# Outputs\n",
    "os.makedirs(path_settings[\"output\"], exist_ok=True)\n",
    "out_dams    = os.path.join(path_settings[\"output\"], f\"{domain}.info_dam.txt\")\n",
    "out_lakes   = os.path.join(path_settings[\"output\"], f\"{domain}.info_lake.txt\")\n",
    "out_sects   = os.path.join(path_settings[\"output\"], f\"{domain}.info_section.txt\")\n",
    "\n",
    "print(\"Domain:\", domain.replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"Data:\", path_settings[\"data\"].replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"Output:\", path_settings[\"output\"].replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"choice:\", choice_path.replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"pnt   :\", pnt_path.replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"dams  :\", dams_file.replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"lakes :\", lakes_file.replace(\"/home/continuumuser/workdir/\",\"\"))\n",
    "print(\"sects :\", sections_file.replace(\"/home/continuumuser/workdir/\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3b56e",
   "metadata": {},
   "source": [
    "#### Read the reference grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649fac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid res: 0.004496608  | x_ll: 39.354901312111004  | y_ul: 14.955939691819001\n"
     ]
    }
   ],
   "source": [
    "# Read the arrays\n",
    "with rio.open(choice_path) as src_choice:\n",
    "    choice = src_choice.read(1)             # np.ndarray\n",
    "    tf = src_choice.transform               # affine transform\n",
    "    left, bottom, right, top = src_choice.bounds\n",
    "\n",
    "with rio.open(pnt_path) as src_pnt:\n",
    "    pnt = src_pnt.read(1)                   # np.ndarray\n",
    "\n",
    "res = abs(tf.a)                             \n",
    "x_ll = float(left - (res / 2))              # min X - half pixel\n",
    "y_ul = float(top  + (res / 2))              # max Y + half pixel\n",
    "\n",
    "print(\"Grid res:\", res, \" | x_ll:\", x_ll, \" | y_ul:\", y_ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae7800",
   "metadata": {},
   "source": [
    "#### Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48609bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_to_hmc(x, y, x_ll, y_ul, res):\n",
    "    Y_HMC = math.ceil(abs((x - x_ll)) / res)\n",
    "    X_HMC = math.ceil(abs((y - y_ul)) / res)\n",
    "    return int(X_HMC), int(Y_HMC)\n",
    "def xy_to_hmc(x, y, tf):\n",
    "    # 0-based coerente con rasterio (floor dagli spigoli)\n",
    "    r0, c0 = rio.transform.rowcol(tf, x, y)\n",
    "    # 1-based dall'alto-sinistra\n",
    "    return int(r0 + 1), int(c0 + 1)\n",
    "def ensure_on_network(choice_arr, X_HMC, Y_HMC, name, kind):\n",
    "    if choice_arr[X_HMC - 1, Y_HMC - 1] < 1 :\n",
    "        raise ValueError(f\"DOMAIN: {domain}. The {kind} '{name}' ({X_HMC}-{Y_HMC}) is not on the network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6276a9f",
   "metadata": {},
   "source": [
    "#### Load points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff01f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ‚Üí dams: File not found | lakes: File not found | sections: File not found\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf_dams = gpd.read_file(dams_file)     if os.path.exists(dams_file)     else None\n",
    "gdf_lakes = gpd.read_file(lakes_file)   if os.path.exists(lakes_file)    else None\n",
    "gdf_sects = gpd.read_file(sections_file)if os.path.exists(sections_file) else None\n",
    "\n",
    "print(\"Loaded ‚Üí dams:\", \"File not found\" if gdf_dams is None else len(gdf_dams),\n",
    "      \"| lakes:\", \"File not found\" if gdf_lakes is None else len(gdf_lakes),\n",
    "      \"| sections:\", \"File not found\" if gdf_sects is None else len(gdf_sects))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae213b",
   "metadata": {},
   "source": [
    "## üíß Generate output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918ce5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dam file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7aea5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dams layer provided.\n"
     ]
    }
   ],
   "source": [
    "DAM_NAME   = dams_lut[\"name_col\"]\n",
    "DAM_YEAR   = dams_lut[\"year_col\"]\n",
    "DAM_CAP    = dams_lut[\"cap_col\"]\n",
    "DAM_LEN    = dams_lut[\"len_col\"]\n",
    "DAM_HGT    = dams_lut[\"height_col\"]\n",
    "DAM_AREA   = dams_lut[\"area_col\"]\n",
    "DAM_DISAVG = dams_lut[\"dis_avg_col\"]\n",
    "\n",
    "if gdf_dams is not None and len(gdf_dams) > 0:\n",
    "    gdf = gdf_dams.copy()\n",
    "    # Normalize names\n",
    "    unnamed_dam_code = 1\n",
    "    index = []\n",
    "    for name in gdf.get(DAM_NAME, [None]*len(gdf)):\n",
    "        if name is not None and str(name).strip():\n",
    "            index.append(str(name))\n",
    "        else:\n",
    "            index.append(f\"Dam_{domain}{str(unnamed_dam_code).zfill(3)}\")\n",
    "            unnamed_dam_code += 1\n",
    "    gdf[DAM_NAME] = index\n",
    "    gdf = gdf.set_index(DAM_NAME)\n",
    "\n",
    "    with open(out_dams, \"w\") as dam_file:\n",
    "        dam_file.write(f\"{len(gdf)}\\t#Number of dams\\n\")\n",
    "        dam_file.write(f\"{len(gdf)}\\t#Number of plants\\n\")\n",
    "        dam_file.write(\"##################################################################################################\\n\")\n",
    "\n",
    "        for dam_name in gdf.index.values:\n",
    "            year_val = gdf.loc[dam_name].get(DAM_YEAR, -9999)\n",
    "            dam_file.write(f\"{dam_name}\\t\\t\\t#Dam name {year_val}\\n\")\n",
    "\n",
    "            X_HMC, Y_HMC = xy_to_hmc(gdf.loc[dam_name].geometry.x, gdf.loc[dam_name].geometry.y, tf)\n",
    "            ensure_on_network(choice, X_HMC, Y_HMC, dam_name, \"dam\")\n",
    "            dam_file.write(f\"{int(X_HMC)} {int(Y_HMC)}\\t\\t\\t#Row and column dam coordinates\\n\")\n",
    "\n",
    "            dam_file.write(\"1\\t\\t\\t#Number of plants downstream the dam\\n\")\n",
    "            dam_file.write(\"-9999\\t\\t\\t#Code of the reservoirs cells of the dam (if point dam set to -9999)\\n\")\n",
    "\n",
    "            max_storage_m3 = float(gdf.loc[dam_name].get(DAM_CAP,0.0)) * (10**6)\n",
    "            initial_storage_m3 = max_storage_m3 * 0.7\n",
    "            dam_file.write(f\"{max_storage_m3}\\t\\t\\t#Max storage (m3)\\n\")\n",
    "            dam_file.write(f\"{initial_storage_m3}\\t\\t\\t#Initial storage (m3)\\n\")\n",
    "\n",
    "            dam_file.write(\"99999\\t\\t\\t#Critical discharge for surface spillway\\n\")\n",
    "            dam_len = float(gdf.loc[dam_name].get(DAM_LEN,0.0))\n",
    "            spillway_len = 0.15 * dam_len if dam_len > 0 else dam_len\n",
    "            dam_file.write(f\"{spillway_len}\\t\\t\\t#Equivalent length of surface spillway\\n\")\n",
    "\n",
    "            max_depth = float(gdf.loc[dam_name].get(DAM_HGT, -1.0))\n",
    "            if max_depth < 0:\n",
    "                area_skm = float(gdf.loc[dam_name].get(DAM_AREA, 1.0))\n",
    "                cap_mcm  = float(gdf.loc[dam_name].get(DAM_CAP, 0.0))\n",
    "                max_depth = round(2 * cap_mcm / area_skm, 0) if area_skm > 0 else 0.0\n",
    "            dam_file.write(f\"{max_depth}\\t\\t\\t#Maximum reservoir depth\\n\")\n",
    "\n",
    "            dam_file.write(\"1e-006\\t\\t\\t#Linear tank coefficient\\n\")\n",
    "            dam_file.write(\"\\t\\t\\t\\t#Depth-volume curve file name\\n\")\n",
    "            dam_file.write(\"\\t\\t\\t\\t#Turbines discharge file name\\n\")\n",
    "\n",
    "            X_HMC_OUT = int(X_HMC) - (int((pnt[int(X_HMC)-1, int(Y_HMC)-1] - 1) / 3) - 1)\n",
    "            Y_HMC_OUT = int(Y_HMC) + pnt[int(X_HMC)-1, int(Y_HMC)-1] - 5 - 3 * (int((pnt[int(X_HMC)-1, int(Y_HMC)-1] - 1) / 3) - 1)\n",
    "            ensure_on_network(choice, X_HMC_OUT, Y_HMC_OUT, dam_name, \"dam outlet\")\n",
    "            dam_file.write(f\"{int(X_HMC_OUT)} {int(Y_HMC_OUT)}\\t\\t\\t#Row and column outlet dam coordinates\\n\")\n",
    "\n",
    "            max_discharge_m3_s =  0.001 * 2.5 * float(gdf.loc[dam_name].get(DAM_DISAVG,0.0))\n",
    "            dam_file.write(\"-9999\\t\\t\\t#Plant corrivation time (minutes)\\n\")\n",
    "            dam_file.write(f\"{max_discharge_m3_s}\\t\\t\\t#Maximum plant discharge (m3/s)\\n\")\n",
    "            dam_file.write(\"1\\t\\t\\t#flag=1 if the plant discharge water\\n\")\n",
    "            dam_file.write(\"##################################################################################################\\n\")\n",
    "    print(\"Written:\", out_dams)\n",
    "else:\n",
    "    print(\"No dams layer provided.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5a020",
   "metadata": {},
   "source": [
    "#### Lakes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e64b952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lakes layer provided.\n"
     ]
    }
   ],
   "source": [
    "LAKE_NAME  = lakes_lut[\"name_col\"]\n",
    "LAKE_REST  = lakes_lut[\"res_time_col\"]\n",
    "LAKE_VOLT  = lakes_lut[\"vol_total_col\"]\n",
    "LAKE_DISA  = lakes_lut[\"dis_avg_col\"]\n",
    "\n",
    "if gdf_lakes is not None and len(gdf_lakes) > 0:\n",
    "    gdf = gdf_lakes.copy()\n",
    "    unnamed_lake_code = 1\n",
    "    index = []\n",
    "    for name in gdf.get(LAKE_NAME, [None]*len(gdf)):\n",
    "        if name is not None and str(name).strip():\n",
    "            index.append(str(name))\n",
    "        else:\n",
    "            index.append(f\"Lake_{domain}{str(unnamed_lake_code).zfill(3)}\")\n",
    "            unnamed_lake_code += 1\n",
    "    gdf[LAKE_NAME] = index\n",
    "    gdf = gdf.set_index(LAKE_NAME)\n",
    "\n",
    "    lakes_not_valid = []\n",
    "    to_drop = gdf.index[gdf.get(LAKE_REST,-1) < 0] if LAKE_REST in gdf.columns else []\n",
    "    if len(to_drop) > 0:\n",
    "        lakes_not_valid += [f\"{domain} : {i}\" for i in to_drop]\n",
    "        gdf = gdf.drop(to_drop)\n",
    "\n",
    "    with open(out_lakes, \"w\") as lake_file:\n",
    "        lake_file.write(f\"{len(gdf)}\\t#Number of lakes\\n\")\n",
    "        lake_file.write(\"##################################################################################################\\n\")\n",
    "\n",
    "        for lake_name in gdf.index.values:\n",
    "            lake_file.write(f\"{lake_name}\\t\\t\\t#Lake name\\n\")           \n",
    "            X_HMC, Y_HMC = xy_to_hmc(gdf.loc[lake_name].geometry.x, gdf.loc[lake_name].geometry.y, tf)\n",
    "            ensure_on_network(choice, X_HMC, Y_HMC, lake_name, \"lake\")\n",
    "            lake_file.write(f\"{int(X_HMC)} {int(Y_HMC)}\\t\\t\\t#Row and column dam coordinates\\n\")\n",
    "            lake_file.write(\"-9999\\t\\t\\t#Code of the lakes cells of the lake (if point dam set to -9999)\\n\")\n",
    "\n",
    "            vol_tot = float(gdf.loc[lake_name].get(LAKE_VOLT,0.0)) * (10**6)\n",
    "            dis_avg = float(gdf.loc[lake_name].get(LAKE_DISA,0.0))\n",
    "            vol_min = 0.0\n",
    "            vol_init = vol_tot\n",
    "\n",
    "            res_time_day = float(gdf.loc[lake_name].get(LAKE_REST,1.0)) if LAKE_REST in gdf.columns else 1.0\n",
    "            res_time_hr  = res_time_day * 24.0\n",
    "            lake_const   = 1.0 / res_time_hr if res_time_hr > 0 else 0.0\n",
    "\n",
    "            lake_file.write(f\"{vol_min}\\t\\t\\t#Minimum storage non-null discharge (m3)\\n\")\n",
    "            lake_file.write(f\"{vol_init}\\t\\t\\t#Initial storage (m3)\\n\")\n",
    "            lake_file.write(f\"{lake_const}\\t\\t\\t#Lake constant (1/h) \\n\")\n",
    "            lake_file.write(\"##################################################################################################\\n\")\n",
    "    if len(lakes_not_valid) > 0:\n",
    "        print(\"WARNING! Lakes skipped because invalid residence time:\", \"; \".join(lakes_not_valid))\n",
    "    print(\"Written:\", out_lakes)\n",
    "else:\n",
    "    print(\"No lakes layer provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a68640",
   "metadata": {},
   "source": [
    "####Sections file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a58924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sections layer provided.\n"
     ]
    }
   ],
   "source": [
    "if gdf_sects is not None and len(gdf_sects) > 0:\n",
    "    gdf = gdf_sects.copy()\n",
    "    # Column names from settings (flat JSON)\n",
    "    name_col  = cfg[\"settings\"][\"sections_name_col\"]\n",
    "    river_col = cfg[\"settings\"][\"sections_river_col\"]\n",
    "\n",
    "    unnamed_code = 1\n",
    "    names = []\n",
    "    for name in gdf.get(name_col, [None]*len(gdf)):\n",
    "        if name:\n",
    "            names.append(str(name).replace(\" \", \"_\"))\n",
    "        else:\n",
    "            names.append(f\"Section_{domain}{str(unnamed_code).zfill(3)}\")\n",
    "            unnamed_code += 1\n",
    "    gdf[name_col] = names\n",
    "    gdf = gdf.set_index(name_col)\n",
    "\n",
    "    with open(out_sects, \"w\") as sec_file:\n",
    "        for sect_name in gdf.index.values:\n",
    "            river = str(gdf.loc[sect_name, river_col]).replace(\" \",\"_\") if river_col in gdf.columns else domain\n",
    "            X_HMC, Y_HMC = xy_to_hmc(gdf.loc[sect_name].geometry.x, gdf.loc[sect_name].geometry.y, tf)\n",
    "            ensure_on_network(choice, X_HMC, Y_HMC, sect_name, \"section\")\n",
    "            sec_file.write(f\"{int(X_HMC)} {int(Y_HMC)} {river} {sect_name}\\n\")\n",
    "    print(\"Written:\", out_sects)\n",
    "else:\n",
    "    print(\"No sections layer provided.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1beaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
